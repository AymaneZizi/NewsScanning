{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_md'\n",
      "Losses {'ner': 5.388405238227268}\n",
      "Losses {'ner': 4.277841001749039}\n",
      "Losses {'ner': 5.1175521535478765}\n",
      "Losses {'ner': 5.210818947636312}\n",
      "Losses {'ner': 3.2898973116712114}\n",
      "Losses {'ner': 6.434330157245313}\n",
      "Losses {'ner': 4.263011075941904}\n",
      "Losses {'ner': 5.500741994881537}\n",
      "Losses {'ner': 3.174374087713659}\n",
      "Losses {'ner': 1.6959528601948097}\n",
      "Losses {'ner': 2.222002437474657}\n",
      "Losses {'ner': 4.0580256544053555}\n",
      "Losses {'ner': 5.416926892516585}\n",
      "Losses {'ner': 3.8612273536491557}\n",
      "Losses {'ner': 0.021234746203903754}\n",
      "Losses {'ner': 0.5560647669044556}\n",
      "Losses {'ner': 0.04269342471343407}\n",
      "Losses {'ner': 0.0005744776651965822}\n",
      "Losses {'ner': 3.5019163525075783}\n",
      "Losses {'ner': 1.9015798980544787}\n",
      "Losses {'ner': 0.10361778683667922}\n",
      "Losses {'ner': 0.01686335855025245}\n",
      "Losses {'ner': 0.013421589173507442}\n",
      "Losses {'ner': 0.0006273040667212904}\n",
      "Losses {'ner': 0.00012199263068168875}\n",
      "Losses {'ner': 0.016159651172529266}\n",
      "Losses {'ner': 1.0490653896615107}\n",
      "Losses {'ner': 0.015399753362430602}\n",
      "Losses {'ner': 0.075029161957616}\n",
      "Losses {'ner': 0.12399757227946917}\n",
      "Losses {'ner': 2.1028923111288123e-05}\n",
      "Losses {'ner': 0.014209310610162618}\n",
      "Losses {'ner': 0.023688339111439305}\n",
      "Losses {'ner': 0.0007619907303464402}\n",
      "Losses {'ner': 0.10910239249805406}\n",
      "Losses {'ner': 3.0965142199690376e-05}\n",
      "Losses {'ner': 5.862965538332094e-06}\n",
      "Losses {'ner': 0.003097269302360983}\n",
      "Losses {'ner': 0.6748761272560841}\n",
      "Losses {'ner': 0.000194184906432286}\n",
      "Losses {'ner': 1.0299663616750825}\n",
      "Losses {'ner': 0.0009348758627122178}\n",
      "Losses {'ner': 0.0001688944849550511}\n",
      "Losses {'ner': 0.0010222993527529822}\n",
      "Losses {'ner': 0.0006586493544833705}\n",
      "Losses {'ner': 3.494519809703789e-06}\n",
      "Losses {'ner': 0.14709205581383955}\n",
      "Losses {'ner': 0.0017019723943421106}\n",
      "Losses {'ner': 1.0910673339032852e-05}\n",
      "Losses {'ner': 0.00010136511648940901}\n",
      "Losses {'ner': 8.055333698250393e-07}\n",
      "Losses {'ner': 8.183385256667689e-07}\n",
      "Losses {'ner': 2.334018169750096e-06}\n",
      "Losses {'ner': 1.2768109574419957e-05}\n",
      "Losses {'ner': 2.5541451811805505e-09}\n",
      "Losses {'ner': 4.889250743850271e-08}\n",
      "Losses {'ner': 1.802876023771161e-05}\n",
      "Losses {'ner': 5.233409794825998e-08}\n",
      "Losses {'ner': 4.0276693838213543e-07}\n",
      "Losses {'ner': 4.951787350139739e-07}\n",
      "Losses {'ner': 8.430553766440017e-06}\n",
      "Losses {'ner': 0.00013054674082398917}\n",
      "Losses {'ner': 1.219892325389789e-06}\n",
      "Losses {'ner': 8.976456180717282e-07}\n",
      "Losses {'ner': 1.2805875858273007e-06}\n",
      "Losses {'ner': 5.369630955312312e-06}\n",
      "Losses {'ner': 0.001028285439781951}\n",
      "Losses {'ner': 1.66779331550813e-06}\n",
      "Losses {'ner': 2.311453360088116e-07}\n",
      "Losses {'ner': 1.136800633113495e-06}\n",
      "Losses {'ner': 2.90751028985887e-07}\n",
      "Losses {'ner': 2.697217723949589e-07}\n",
      "Losses {'ner': 3.348509437334094e-07}\n",
      "Losses {'ner': 2.2791686300113655e-08}\n",
      "Losses {'ner': 5.2613781454033336e-12}\n",
      "Losses {'ner': 0.0005678926378417375}\n",
      "Losses {'ner': 5.508705659468973e-05}\n",
      "Losses {'ner': 1.3580391629323333e-09}\n",
      "Losses {'ner': 2.30026009665764e-05}\n",
      "Losses {'ner': 0.00010818484044678237}\n",
      "Losses {'ner': 1.4403249366819366e-05}\n",
      "Losses {'ner': 5.395028894010501e-07}\n",
      "Losses {'ner': 0.0004998576272099432}\n",
      "Losses {'ner': 1.321751054029933e-06}\n",
      "Losses {'ner': 0.0024860405125085336}\n",
      "Losses {'ner': 2.483358164719491e-09}\n",
      "Losses {'ner': 9.855559475099973e-05}\n",
      "Losses {'ner': 5.878103514174514e-09}\n",
      "Losses {'ner': 5.936184333171602e-12}\n",
      "Losses {'ner': 3.131690613163648e-09}\n",
      "Losses {'ner': 1.834384442329919}\n",
      "Losses {'ner': 2.476657283846046e-12}\n",
      "Losses {'ner': 2.1203142277704907e-10}\n",
      "Losses {'ner': 1.7021467512657176e-08}\n",
      "Losses {'ner': 2.390322011763597e-07}\n",
      "Losses {'ner': 3.0230812338909867e-06}\n",
      "Losses {'ner': 6.599767384114345e-05}\n",
      "Losses {'ner': 3.964180524096937e-10}\n",
      "Losses {'ner': 2.2576785584572554e-09}\n",
      "Losses {'ner': 7.053897758808802e-10}\n",
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
      "Saved model to data\\nlp\\custom_en\n",
      "Loading from data\\nlp\\custom_en\n",
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "Last tested with: v2.1.0\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
    "]\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly â€“ but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('en_core_web_md','data/nlp/custom_en')\n",
    "    \n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
